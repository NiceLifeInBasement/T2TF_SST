Upcoming work that has to be done:

SIMULATION OF DATA
	change the variance for every sensor and have the covariance algo know that (therefore changing
	their respective covariance matrices in such a way that the sensor with higher variance will get a lower weight)
		already implemented a simple way of doing this, maybe redo it (based on variance lists instead of linear change)

	the entire code needs some cleanup re: covariance simulation
	some functions still get cov_example_id and inside they have hard coded max/min_val for get_random_cov (i.e. the spread fct)
		

REAL DATA	
	c2x vehicle display is not working correctly (jumping around etc)
		--> fixed for the most part by implementing fake data + velocity rotation 
		there are still some issues left, the c2x data data doesn't match the lidar good enough for association in scenarios
		where cars are very close to each other
		--> Added in a flat offset for the x-axis (on the c2x data, value ~4)
			This drastically improved the results in both bags.
			However, it might be "overfitting" to the given data
			This is also not done in the algorithms, but alongside the transformation in the main node

	re: creation of fake data to fill time steps where no c2x data was received:
	currently, timing and number of fake data points are fixed (+4 points with 0.04 sec delay between)
	you could also base this on the timing diff between the last "real" c2x data that was received and the new data
	formula would be~:
		time_diff = 0.2  # This is the case for the "normal" transmission rate of 5Hz
		# Calc time_diff from the data
		no_points = 5  # This could be a constant
		add_points = no_points - 1
		time_shift = time_diff/no_points
		# add_points and time_shift are the two variables used in the code 
	You could also add these points "into the past" which would let you calc time_diff accuratly
	Big Downside: Would be based on the assumption that c2x data actually arrives faster than lidar data
		
	c2x buffering:
		use a queue instead of an array, so that you don't waste your time looking through already-used entries
		not sure how much impact that has, but it def. is far from optimal atm.
		--> implemented: delete used entries from the c2x array
			this was not yet implemented for the history object, TODO check if entries there are used multiple times
			(in that case, you can't simply remove them after extracting them like in the c2x data)

	work on profiling to see what exactly is causing lag etc 
		np.linalg.inv is something I have seen multiple times already
		in general, so far profiling didn't help pinpointing the issue


GENERAL IMPROVEMENTS 
	consider changing the visualization so that instead of plotting directly, the data is just merged at a fusion center and
	then published to a new topic. A second node can then subscribe to this topic and simply display ALL information from
	the topic
		topic could have its own msg format, that simply includes an array of tuples that match what visuals needs
		(i.e. (x,y,id,color))
		so msg would just be header+points, with points being an array of x+y+id+color (which in itself should probably be
		a second msg definition)
		this would also allow you to store bagfiles of results

	consider adding in (/testing) the following change to the t2td(with history) function:
	1.	instead of averaging over all time points included in the history, weigh the more recent ones higher
		need to think about a weighting and how much sense this makes
	2.	a dynamic threshold, that slowly increases as long as no object match was found, and decreases while objects are 
		matched (this might be not necessary, selecting a decent threshold should be enough)

OTHER DATASETS
	NuScenes
	Pro: 	Very good data, annotated, lots of meaningful scenes, different sensors 
	Issue: 	Technically, the goal was to "ignore" the used sensor and work on finished tracks instead
		Scenes are for a single car, the goal implied the usage of multiple cars that drive along side each other though.



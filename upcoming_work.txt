### Upcoming work that has to be done:

speed up the entire code? (maybe enable dropping the "normal" visuals in favor of pure rviz visualization?)
	also test if the markerarray generation causes performance loss?

improve the display of association results in rviz
	any way to display only a single point instead of overlaying both points all the time?

consider adding in a fusion and publishing the fusion results instead of "just" association results
	"fusion" --> average and display that instead of the pure results, also gets rid of the double visualization

add the visualization in rviz
	maybe by publishing a marker array
	works

potential improvements to the T2TA algorithm?
	- distance calc:
		if you use velocity + position for distance, then the velocity will have lower impact than position (assuming that 
		positional distance will usually be larger than velocity diff)
		--> potentially come up with a different distance measurement to be used

	- does not involve gating at all
		matrix size could be reduced if gating was somehow implemented in the algo

# ------------------------
# New Data (13. Dec)
# ------------------------
seems to be working quite ok

currently converting viewcar2 -> odom and that into fascare.ibeo_front_center

TODO:
	- try to plot ONLY the associated objects (or the other opaque without id? idk) to see if mostly vehicles are associated
		cant really tell. While the pointclouds overlap (as the cars pass each other) a lot of other objects are associated
	- check if the conversion makes sense (see above) or if different frames need to be used
		Seems to make sense, but with the given data its really hard to tell
				when plotting the 0/0 point of the viewcar2 object:
		it jumps around A LOT (large distances aswell)
		there is probably some issue that could be related to the conversion (or the measurements in general)
	- check for lag (lag def. exists, but without history and with the new scatter fct its bearable)

	- consider adding in an additional plot that plots 0/0 in dark blue and the center of the other objects in yellow
		so laser objects are blue/yellow, but opaque
		ego positon is blue/yellow with alpha=1
		associated objects are red

		--> implemeted with the boolean do_ego_plot (in setup)
# ------------------------
# Upcoming 11.Dec
# (in descending order)
# ------------------------

[data coming next week hopefully]
	Test the historic association algorithm for this data
	
Simulate additional CAM messages to the real data
	Get a fixed scenario so that you know which IDs match cars and which don't
	Create a simulated CAM message for that car, that has measurement error (covariance is not needed for now)
	Test the association algorithm for this new data

	--> WORKS

Kalman filter to estimate the offset (the one currently fixed to 4)
	or another method that has a good mathematical foundation for estimating the offset
	lower priority
	do some literature research to check what commonly used method for this problem are

	DONT DO THIS FOR NOW

# ------------------------
# Pre 11.Dec 
# ------------------------

SIMULATION OF DATA
	change the variance for every sensor and have the covariance algo know that (therefore changing
	their respective covariance matrices in such a way that the sensor with higher variance will get a lower weight)
		already implemented a simple way of doing this, maybe redo it (based on variance lists instead of linear change)

		--> Done, currently just a linear shift across all "measurement nodes"

	the entire code needs some cleanup re: covariance simulation
	some functions still get cov_example_id and inside they have hard coded max/min_val for get_random_cov (i.e. the spread fct)
		

REAL DATA	
	c2x vehicle display is not working correctly (jumping around etc)
		--> fixed for the most part by impl   ementing fake data + velocity rotation 
		there are still some issues left, the c2x data data doesn't match the lidar good enough for association in scenarios
		where cars are very close to each other
		--> Added in a flat offset for the x-axis (on the c2x data, value ~4)
			This drastically improved the results in both bags.
			However, it might be "overfitting" to the given data
			This is also not done in the algorithms, but alongside the transformation in the main node

	re: creation of fake data to fill time steps where no c2x data was received:
	currently, timing and number of fake data points are fixed (+4 points with 0.04 sec delay between)
	you could also base this on the timing diff between the last "real" c2x data that was received and the new data
	formula would be~:
		time_diff = 0.2  # This is the case for the "normal" transmission rate of 5Hz
		# Calc time_diff from the data
		no_points = 5  # This could be a constant
		add_points = no_points - 1
		time_shift = time_diff/no_points
		# add_points and time_shift are the two variables used in the code 
	You could also add these points "into the past" which would let you calc time_diff accuratly
	Big Downside: Would be based on the assumption that c2x data actually arrives faster than lidar data
		
	c2x buffering:
		use a queue instead of an array, so that you don't waste your time looking through already-used entries
		not sure how much impact that has, but it def. is far from optimal atm.
		--> implemented: delete used entries from the c2x array
			this was not yet implemented for the history object, TODO check if entries there are used multiple times
			(in that case, you can't simply remove them after extracting them like in the c2x data)

	work on profiling to see what exactly is causing lag etc 
		np.linalg.inv is something I have seen multiple times already - but it doesnt really have high timing
		in general, so far profiling didn't help pinpointing the issue

	lag could be caused by the locking in combination with the general computation time of the algorithm preventing data
	from coming in during those times
	
	the indexerror during assoc probably needs some fixing, it's not only happening at the beginning (?)


GENERAL IMPROVEMENTS 
	consider changing the visualization so that instead of plotting directly, the data is just merged at a fusion center and
	then published to a new topic. A second node can then subscribe to this topic and simply display ALL information from
	the topic
		topic could have its own msg format, that simply includes an array of tuples that match what visuals needs
		(i.e. (x,y,id,color))
		so msg would just be header+points, with points being an array of x+y+id+color (which in itself should probably be
		a second msg definition)
		this would also allow you to store bagfiles of results
	You could also clean up the visualization and remove the reliance on different lists for x/y coords etc and instead simply
	reduce everything to tuple-based storage/display (since thats far more useful in practice anyway)

	consider adding in (/testing) the following change to the t2td(with history) function:
	1.	instead of averaging over all time points included in the history, weigh the more recent ones higher
		need to think about a weighting and how much sense this makes
	2.	a dynamic threshold, that slowly increases as long as no object match was found, and decreases while objects are 
		matched (this might be not necessary, selecting a decent threshold should be enough)

FUTURE WORK/IDEAS
	use information fusion after the t2ta process, but not necessarily to improve positional data etc, but instead to provide
	a better view on the data.
	This is well-suited for incorporating CPM messages
	(each word represents a car in the following examples)
	example-1: 	EGO   LIDAR-1   C2X   			
  				   LIDAR-2   
		In this scenario, the c2x car will not show up on the lidar track. Fusing the given information therefore yields
		a better view on the current road situation. This is straightforward fusion

	example-2:
			EGO	C2X			LIDAR-1
					LIDAR-2
		In this scenario, the C2X car is shown to be infront of the ego vehicle, but no lidar match is found for it.
		This indicates an issue with either the lidar tracker or the c2x objects positional data.
		(the view of the c2x should not be obstruced by any object, yet no match is found. Another indicator for an error
		in this situation is the fact that the LIDAR-1 object is shown behind the c2x object - apparantly nothing is 
		obstructing the lidar on the this path, yet the c2x object is not visible. This probably indicates an error with the
		c2x data.

	---

	predict c2x maneuvers (passing etc)
		might be a pretty big task
		might not be as great with cpm insted of cam messages	

		probably needs a lot more data to prevent overfitting
		
	---
	
	road boundary prediction?
		already discussed earlier, probably too hard
		if ibeos inbuilt tracker includes classification this is unnecessary

	---
	
	currently, a fixed offset of 4 is used. 
	assuming that such fixed offset for the c2x data exists, you could try to dynamically optimize this value
		example: while an association is made, the x+y offsets are changed by a little bit in every timestep, in such a way
		that all associations improve the most.
		This should after some time yield a close-to-optimal offset (assuming that it exists (+ is linear) for all points)
		REFER TO https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6728345		
			key difference: here, rotation and translation is acquired by means of point matching, instead of just
					improved
	alternativly, work with a per-object offset (every object that was associated at least once gets such an offset)
   
		--> Implemented this, but you should not be doing this.
			Even the fixed offset would be better (easier to explain) than this

OTHER DATASETS
	NuScenes
	Pro: 	Very good data, annotated, lots of meaningful scenes, different sensors 
	Issue: 	Technically, the goal was to "ignore" the used sensor and work on finished tracks instead
		Scenes are for a single car, the goal implied the usage of multiple cars that drive along side each other though.
		Fusion between different sensors was already performed here.

